{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-05-13T07:30:26.495027Z",
     "iopub.status.busy": "2025-05-13T07:30:26.494423Z",
     "iopub.status.idle": "2025-05-13T07:30:29.438617Z",
     "shell.execute_reply": "2025-05-13T07:30:29.437630Z",
     "shell.execute_reply.started": "2025-05-13T07:30:26.494999Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sacrebleu in /usr/local/lib/python3.11/dist-packages (2.5.1)\n",
      "Requirement already satisfied: sacremoses in /usr/local/lib/python3.11/dist-packages (0.1.1)\n",
      "Requirement already satisfied: portalocker in /usr/local/lib/python3.11/dist-packages (from sacrebleu) (3.1.1)\n",
      "Requirement already satisfied: regex in /usr/local/lib/python3.11/dist-packages (from sacrebleu) (2024.11.6)\n",
      "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.11/dist-packages (from sacrebleu) (0.9.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from sacrebleu) (1.26.4)\n",
      "Requirement already satisfied: colorama in /usr/local/lib/python3.11/dist-packages (from sacrebleu) (0.4.6)\n",
      "Requirement already satisfied: lxml in /usr/local/lib/python3.11/dist-packages (from sacrebleu) (5.3.1)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from sacremoses) (8.1.8)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from sacremoses) (1.4.2)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from sacremoses) (4.67.1)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->sacrebleu) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->sacrebleu) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->sacrebleu) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->sacrebleu) (2025.1.0)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->sacrebleu) (2022.1.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->sacrebleu) (2.4.1)\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->sacrebleu) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->sacrebleu) (2022.1.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->sacrebleu) (1.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->sacrebleu) (2024.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->sacrebleu) (2024.2.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install sacrebleu sacremoses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-13T07:30:29.440443Z",
     "iopub.status.busy": "2025-05-13T07:30:29.440165Z",
     "iopub.status.idle": "2025-05-13T07:30:41.502665Z",
     "shell.execute_reply": "2025-05-13T07:30:41.502079Z",
     "shell.execute_reply.started": "2025-05-13T07:30:29.440421Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from transformers import MarianMTModel, MarianTokenizer\n",
    "import torch\n",
    "\n",
    "# Load model and tokenizer\n",
    "model_name = \"/kaggle/input/full-train-inshallah/opus-mt-tc-big-en-ar-finetuned-en-to-ar/checkpoint-14062\"\n",
    "tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
    "model = MarianMTModel.from_pretrained(model_name)\n",
    "\n",
    "# Use GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# Beam search settings\n",
    "num_beams = 1\n",
    "length_penalty = 1.0\n",
    "batch_size = 32\n",
    "\n",
    "def chunkify(lst, n):\n",
    "    \"\"\"Yield successive n-sized chunks from list.\"\"\"\n",
    "    for i in range(0, len(lst), n):\n",
    "        yield lst[i:i + n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-13T07:30:41.503593Z",
     "iopub.status.busy": "2025-05-13T07:30:41.503341Z",
     "iopub.status.idle": "2025-05-13T07:30:41.507185Z",
     "shell.execute_reply": "2025-05-13T07:30:41.506479Z",
     "shell.execute_reply.started": "2025-05-13T07:30:41.503570Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "times = [1,2,3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-13T07:30:41.508975Z",
     "iopub.status.busy": "2025-05-13T07:30:41.508801Z",
     "iopub.status.idle": "2025-05-13T07:31:56.271100Z",
     "shell.execute_reply": "2025-05-13T07:31:56.270463Z",
     "shell.execute_reply.started": "2025-05-13T07:30:41.508962Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting batched translation of the sentences...\n",
      "Translation completed in 74.71 seconds.\n",
      "Saved to: /kaggle/working/tatoeba-translated.txt\n"
     ]
    }
   ],
   "source": [
    "input_file = \"/kaggle/input/bleu-test/test.txt\"\n",
    "output_file = \"/kaggle/working/tatoeba-translated.txt\"\n",
    "\n",
    "print(\"Starting batched translation of the sentences...\")\n",
    "\n",
    "with open(input_file, \"r\", encoding=\"utf-8\") as infile:\n",
    "    sentences = [line.strip() for line in infile if line.strip()]\n",
    "\n",
    "start_time = time.time()\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as outfile:\n",
    "    for batch_num, batch in enumerate(chunkify(sentences, batch_size), 1):\n",
    "        try:\n",
    "            encoded = tokenizer(batch, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "            encoded = {k: v.to(device) for k, v in encoded.items()}\n",
    "\n",
    "            outputs = model.generate(\n",
    "                **encoded,\n",
    "                num_beams=num_beams,\n",
    "                length_penalty=length_penalty,\n",
    "            )\n",
    "\n",
    "            decoded = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "            outfile.write('\\n'.join(decoded) + '\\n')\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error in batch {batch_num}: {e}\")\n",
    "            for _ in batch:\n",
    "                outfile.write(\"Error in translation\\n\")\n",
    "\n",
    "end_time = time.time()\n",
    "times[0] = end_time - start_time\n",
    "print(f\"Translation completed in {(times[0]):.2f} seconds.\")\n",
    "print(f\"Saved to: {output_file}\")\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.ipc_collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-13T07:31:56.272011Z",
     "iopub.status.busy": "2025-05-13T07:31:56.271806Z",
     "iopub.status.idle": "2025-05-13T07:32:13.276459Z",
     "shell.execute_reply": "2025-05-13T07:32:13.275682Z",
     "shell.execute_reply.started": "2025-05-13T07:31:56.271988Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting batched translation of the sentences...\n",
      "Translation completed in 16.97 seconds.\n",
      "Saved to: /kaggle/working/flores101-translated.txt\n"
     ]
    }
   ],
   "source": [
    "input_file = \"/kaggle/input/flores101-enar/eng.devtest\"\n",
    "output_file = \"/kaggle/working/flores101-translated.txt\"\n",
    "\n",
    "print(\"Starting batched translation of the sentences...\")\n",
    "\n",
    "with open(input_file, \"r\", encoding=\"utf-8\") as infile:\n",
    "    sentences = [line.strip() for line in infile if line.strip()]\n",
    "\n",
    "start_time = time.time()\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as outfile:\n",
    "    for batch_num, batch in enumerate(chunkify(sentences, batch_size), 1):\n",
    "        try:\n",
    "            encoded = tokenizer(batch, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "            encoded = {k: v.to(device) for k, v in encoded.items()}\n",
    "\n",
    "            outputs = model.generate(\n",
    "                **encoded,\n",
    "                num_beams=num_beams,\n",
    "                length_penalty=length_penalty,\n",
    "            )\n",
    "\n",
    "            decoded = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "            outfile.write('\\n'.join(decoded) + '\\n')\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error in batch {batch_num}: {e}\")\n",
    "            for _ in batch:\n",
    "                outfile.write(\"Error in translation\\n\")\n",
    "\n",
    "end_time = time.time()\n",
    "times[1] = end_time - start_time\n",
    "print(f\"Translation completed in {(times[1]):.2f} seconds.\")\n",
    "print(f\"Saved to: {output_file}\")\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.ipc_collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-13T07:32:13.277564Z",
     "iopub.status.busy": "2025-05-13T07:32:13.277305Z",
     "iopub.status.idle": "2025-05-13T07:33:16.292882Z",
     "shell.execute_reply": "2025-05-13T07:33:16.292257Z",
     "shell.execute_reply.started": "2025-05-13T07:32:13.277546Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting batched translation of the sentences...\n",
      "Translation completed in 62.85 seconds.\n",
      "Saved to: /kaggle/working/tico-translated.txt\n"
     ]
    }
   ],
   "source": [
    "input_file = \"/kaggle/input/tico19-enar/test.en\"\n",
    "output_file = \"/kaggle/working/tico-translated.txt\"\n",
    "\n",
    "print(\"Starting batched translation of the sentences...\")\n",
    "\n",
    "with open(input_file, \"r\", encoding=\"utf-8\") as infile:\n",
    "    sentences = [line.strip() for line in infile if line.strip()]\n",
    "\n",
    "start_time = time.time()\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as outfile:\n",
    "    for batch_num, batch in enumerate(chunkify(sentences, batch_size), 1):\n",
    "        try:\n",
    "            encoded = tokenizer(batch, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "            encoded = {k: v.to(device) for k, v in encoded.items()}\n",
    "\n",
    "            outputs = model.generate(\n",
    "                **encoded,\n",
    "                num_beams=num_beams,\n",
    "                length_penalty=length_penalty,\n",
    "            )\n",
    "\n",
    "            decoded = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "            outfile.write('\\n'.join(decoded) + '\\n')\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error in batch {batch_num}: {e}\")\n",
    "            for _ in batch:\n",
    "                outfile.write(\"Error in translation\\n\")\n",
    "\n",
    "end_time = time.time()\n",
    "times[2] = end_time - start_time\n",
    "print(f\"Translation completed in {(times[2]):.2f} seconds.\")\n",
    "print(f\"Saved to: {output_file}\")\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.ipc_collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-13T07:33:16.293859Z",
     "iopub.status.busy": "2025-05-13T07:33:16.293595Z",
     "iopub.status.idle": "2025-05-13T07:33:28.750016Z",
     "shell.execute_reply": "2025-05-13T07:33:28.749392Z",
     "shell.execute_reply.started": "2025-05-13T07:33:16.293831Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model:/kaggle/input/full-train-inshallah/opus-mt-tc-big-en-ar-finetuned-en-to-ar/checkpoint-14062\n",
      "testset                 \t  BLEU\t  chr-F\t  TER\tTime\n",
      "tatoeba-test-v2021-08-07\t  23.4\t   50.7\t 62.5\t74.71s\n",
      "flores101-devtest       \t  29.4\t   60.1\t 54.9\t16.97s\n",
      "tico19-test             \t  30.9\t   59.9\t 56.1\t62.85s\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import json\n",
    "from tabulate import tabulate\n",
    "\n",
    "# Define test sets and pre-recorded times\n",
    "tests = [\n",
    "    {\n",
    "        \"name\": \"tatoeba-test-v2021-08-07\",\n",
    "        \"ref\": \"/kaggle/input/bleu-test/ref1.txt\",\n",
    "        \"hyp\": \"/kaggle/working/tatoeba-translated.txt\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"flores101-devtest\",\n",
    "        \"ref\": \"/kaggle/input/flores101-enar/ara.devtest\",\n",
    "        \"hyp\": \"/kaggle/working/flores101-translated.txt\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"tico19-test\",\n",
    "        \"ref\": \"/kaggle/input/tico19-enar/ref.ar\",\n",
    "        \"hyp\": \"/kaggle/working/tico-translated.txt\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# Function to run sacrebleu and extract metrics\n",
    "def get_scores_json(ref, hyp):\n",
    "    result = subprocess.run(\n",
    "        [\"sacrebleu\", ref, \"-i\", hyp, \"-m\", \"bleu\", \"chrf\", \"ter\"],\n",
    "        capture_output=True,\n",
    "        text=True\n",
    "    )\n",
    "    if result.returncode != 0:\n",
    "        print(f\"Error: {result.stderr}\")\n",
    "        return None\n",
    "    try:\n",
    "        data = json.loads(result.stdout)\n",
    "        scores = {entry[\"name\"]: entry[\"score\"] for entry in data}\n",
    "        return scores\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to parse JSON: {e}\")\n",
    "        return None\n",
    "\n",
    "# Build table\n",
    "full_table = []\n",
    "\n",
    "for i, test in enumerate(tests):\n",
    "    scores = get_scores_json(test[\"ref\"], test[\"hyp\"])\n",
    "    time_str = f\"{(times[i]):.2f}s\"\n",
    "    if scores:\n",
    "        full_table.append([\n",
    "            test[\"name\"],\n",
    "            f\"{scores.get('BLEU', 0):.1f}\",\n",
    "            f\"{scores.get('chrF2', 0):.1f}\",\n",
    "            f\"{scores.get('TER', 0):.1f}\",\n",
    "            time_str\n",
    "        ])\n",
    "    else:\n",
    "        full_table.append([test[\"name\"], \"N/A\", \"N/A\", \"N/A\", time_str])\n",
    "\n",
    "# Print result\n",
    "print(f\"Model:{model_name}\")\n",
    "headers = [\"testset\", \"BLEU\", \"chr-F\", \"TER\", \"Time\"]\n",
    "print(tabulate(full_table, headers=headers, tablefmt=\"tsv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 6646653,
     "sourceId": 10722250,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6657619,
     "sourceId": 10737194,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6657624,
     "sourceId": 11663036,
     "sourceType": "datasetVersion"
    },
    {
     "sourceId": 224722795,
     "sourceType": "kernelVersion"
    }
   ],
   "dockerImageVersionId": 31012,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

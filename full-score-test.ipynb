{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-05-13T07:21:22.387004Z",
     "iopub.status.busy": "2025-05-13T07:21:22.386837Z",
     "iopub.status.idle": "2025-05-13T07:21:27.238897Z",
     "shell.execute_reply": "2025-05-13T07:21:27.238174Z",
     "shell.execute_reply.started": "2025-05-13T07:21:22.386988Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sacrebleu\n",
      "  Downloading sacrebleu-2.5.1-py3-none-any.whl.metadata (51 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.8/51.8 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting sacremoses\n",
      "  Downloading sacremoses-0.1.1-py3-none-any.whl.metadata (8.3 kB)\n",
      "Collecting portalocker (from sacrebleu)\n",
      "  Downloading portalocker-3.1.1-py3-none-any.whl.metadata (8.6 kB)\n",
      "Requirement already satisfied: regex in /usr/local/lib/python3.11/dist-packages (from sacrebleu) (2024.11.6)\n",
      "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.11/dist-packages (from sacrebleu) (0.9.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from sacrebleu) (1.26.4)\n",
      "Requirement already satisfied: colorama in /usr/local/lib/python3.11/dist-packages (from sacrebleu) (0.4.6)\n",
      "Requirement already satisfied: lxml in /usr/local/lib/python3.11/dist-packages (from sacrebleu) (5.3.1)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from sacremoses) (8.1.8)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from sacremoses) (1.4.2)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from sacremoses) (4.67.1)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->sacrebleu) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->sacrebleu) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->sacrebleu) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->sacrebleu) (2025.1.0)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->sacrebleu) (2022.1.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->sacrebleu) (2.4.1)\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->sacrebleu) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->sacrebleu) (2022.1.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->sacrebleu) (1.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->sacrebleu) (2024.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->sacrebleu) (2024.2.0)\n",
      "Downloading sacrebleu-2.5.1-py3-none-any.whl (104 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.1/104.1 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading sacremoses-0.1.1-py3-none-any.whl (897 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m897.5/897.5 kB\u001b[0m \u001b[31m21.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading portalocker-3.1.1-py3-none-any.whl (19 kB)\n",
      "Installing collected packages: sacremoses, portalocker, sacrebleu\n",
      "Successfully installed portalocker-3.1.1 sacrebleu-2.5.1 sacremoses-0.1.1\n"
     ]
    }
   ],
   "source": [
    "!pip install sacrebleu sacremoses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-13T07:21:27.240005Z",
     "iopub.status.busy": "2025-05-13T07:21:27.239780Z",
     "iopub.status.idle": "2025-05-13T07:22:01.905356Z",
     "shell.execute_reply": "2025-05-13T07:22:01.904751Z",
     "shell.execute_reply.started": "2025-05-13T07:21:27.239985Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-13 07:21:41.553784: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1747120901.768823      31 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1747120901.835294      31 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea3defd7c77a45da9f86be088bf8d110",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/337 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b3f0d1128c44eeda172e5015e2f985e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "source.spm:   0%|          | 0.00/806k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f85d37634ce404a9021183b7cefbe90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "target.spm:   0%|          | 0.00/916k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f04e824acf2d4be5822b76c1c5c42ea1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/2.21M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b10eb55d9c64f4bb8422c4959dba7bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/65.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28f94a23b04b49d4b97d34ed42daed76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.08k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87e8d80ae8c24511a4f531f3a46c4791",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/478M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "927ac035ccd44a9fa338d86981cc342d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/301 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import time\n",
    "from transformers import MarianMTModel, MarianTokenizer\n",
    "import torch\n",
    "\n",
    "# Load model and tokenizer\n",
    "model_name = \"Helsinki-NLP/opus-mt-tc-big-en-ar\"\n",
    "tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
    "model = MarianMTModel.from_pretrained(model_name)\n",
    "\n",
    "# Use GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# Beam search settings\n",
    "num_beams = 1\n",
    "length_penalty = 1.0\n",
    "batch_size = 32\n",
    "\n",
    "def chunkify(lst, n):\n",
    "    \"\"\"Yield successive n-sized chunks from list.\"\"\"\n",
    "    for i in range(0, len(lst), n):\n",
    "        yield lst[i:i + n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-13T07:25:12.575000Z",
     "iopub.status.busy": "2025-05-13T07:25:12.574737Z",
     "iopub.status.idle": "2025-05-13T07:25:12.578600Z",
     "shell.execute_reply": "2025-05-13T07:25:12.577942Z",
     "shell.execute_reply.started": "2025-05-13T07:25:12.574979Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "times = [1,2,3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-13T07:25:12.962929Z",
     "iopub.status.busy": "2025-05-13T07:25:12.962738Z",
     "iopub.status.idle": "2025-05-13T07:26:32.227069Z",
     "shell.execute_reply": "2025-05-13T07:26:32.226248Z",
     "shell.execute_reply.started": "2025-05-13T07:25:12.962914Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting batched translation of the sentences...\n",
      "Translation completed in 79.03 seconds.\n",
      "Saved to: /kaggle/working/tatoeba-translated.txt\n"
     ]
    }
   ],
   "source": [
    "input_file = \"/kaggle/input/bleu-test/test.txt\"\n",
    "output_file = \"/kaggle/working/tatoeba-translated.txt\"\n",
    "\n",
    "print(\"Starting batched translation of the sentences...\")\n",
    "\n",
    "with open(input_file, \"r\", encoding=\"utf-8\") as infile:\n",
    "    sentences = [line.strip() for line in infile if line.strip()]\n",
    "\n",
    "start_time = time.time()\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as outfile:\n",
    "    for batch_num, batch in enumerate(chunkify(sentences, batch_size), 1):\n",
    "        try:\n",
    "            encoded = tokenizer(batch, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "            encoded = {k: v.to(device) for k, v in encoded.items()}\n",
    "\n",
    "            outputs = model.generate(\n",
    "                **encoded,\n",
    "                num_beams=num_beams,\n",
    "                length_penalty=length_penalty,\n",
    "            )\n",
    "\n",
    "            decoded = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "            outfile.write('\\n'.join(decoded) + '\\n')\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error in batch {batch_num}: {e}\")\n",
    "            for _ in batch:\n",
    "                outfile.write(\"Error in translation\\n\")\n",
    "\n",
    "end_time = time.time()\n",
    "times[0] = end_time - start_time\n",
    "print(f\"Translation completed in {(times[0]):.2f} seconds.\")\n",
    "print(f\"Saved to: {output_file}\")\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.ipc_collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-13T07:26:32.228868Z",
     "iopub.status.busy": "2025-05-13T07:26:32.228457Z",
     "iopub.status.idle": "2025-05-13T07:26:48.520562Z",
     "shell.execute_reply": "2025-05-13T07:26:48.519750Z",
     "shell.execute_reply.started": "2025-05-13T07:26:32.228842Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting batched translation of the sentences...\n",
      "Translation completed in 16.25 seconds.\n",
      "Saved to: /kaggle/working/flores101-translated.txt\n"
     ]
    }
   ],
   "source": [
    "input_file = \"/kaggle/input/flores101-enar/eng.devtest\"\n",
    "output_file = \"/kaggle/working/flores101-translated.txt\"\n",
    "\n",
    "print(\"Starting batched translation of the sentences...\")\n",
    "\n",
    "with open(input_file, \"r\", encoding=\"utf-8\") as infile:\n",
    "    sentences = [line.strip() for line in infile if line.strip()]\n",
    "\n",
    "start_time = time.time()\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as outfile:\n",
    "    for batch_num, batch in enumerate(chunkify(sentences, batch_size), 1):\n",
    "        try:\n",
    "            encoded = tokenizer(batch, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "            encoded = {k: v.to(device) for k, v in encoded.items()}\n",
    "\n",
    "            outputs = model.generate(\n",
    "                **encoded,\n",
    "                num_beams=num_beams,\n",
    "                length_penalty=length_penalty,\n",
    "            )\n",
    "\n",
    "            decoded = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "            outfile.write('\\n'.join(decoded) + '\\n')\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error in batch {batch_num}: {e}\")\n",
    "            for _ in batch:\n",
    "                outfile.write(\"Error in translation\\n\")\n",
    "\n",
    "end_time = time.time()\n",
    "times[1] = end_time - start_time\n",
    "print(f\"Translation completed in {(times[1]):.2f} seconds.\")\n",
    "print(f\"Saved to: {output_file}\")\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.ipc_collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-13T07:26:48.522135Z",
     "iopub.status.busy": "2025-05-13T07:26:48.521420Z",
     "iopub.status.idle": "2025-05-13T07:27:41.810104Z",
     "shell.execute_reply": "2025-05-13T07:27:41.809450Z",
     "shell.execute_reply.started": "2025-05-13T07:26:48.522107Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting batched translation of the sentences...\n",
      "Translation completed in 53.23 seconds.\n",
      "Saved to: /kaggle/working/tico-translated.txt\n"
     ]
    }
   ],
   "source": [
    "input_file = \"/kaggle/input/tico19-enar/test.en\"\n",
    "output_file = \"/kaggle/working/tico-translated.txt\"\n",
    "\n",
    "print(\"Starting batched translation of the sentences...\")\n",
    "\n",
    "with open(input_file, \"r\", encoding=\"utf-8\") as infile:\n",
    "    sentences = [line.strip() for line in infile if line.strip()]\n",
    "\n",
    "start_time = time.time()\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as outfile:\n",
    "    for batch_num, batch in enumerate(chunkify(sentences, batch_size), 1):\n",
    "        try:\n",
    "            encoded = tokenizer(batch, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "            encoded = {k: v.to(device) for k, v in encoded.items()}\n",
    "\n",
    "            outputs = model.generate(\n",
    "                **encoded,\n",
    "                num_beams=num_beams,\n",
    "                length_penalty=length_penalty,\n",
    "            )\n",
    "\n",
    "            decoded = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "            outfile.write('\\n'.join(decoded) + '\\n')\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error in batch {batch_num}: {e}\")\n",
    "            for _ in batch:\n",
    "                outfile.write(\"Error in translation\\n\")\n",
    "\n",
    "end_time = time.time()\n",
    "times[2] = end_time - start_time\n",
    "print(f\"Translation completed in {(times[2]):.2f} seconds.\")\n",
    "print(f\"Saved to: {output_file}\")\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.ipc_collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-13T07:27:41.812015Z",
     "iopub.status.busy": "2025-05-13T07:27:41.811814Z",
     "iopub.status.idle": "2025-05-13T07:27:54.180792Z",
     "shell.execute_reply": "2025-05-13T07:27:54.180015Z",
     "shell.execute_reply.started": "2025-05-13T07:27:41.812000Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model:Helsinki-NLP/opus-mt-tc-big-en-ar\n",
      "testset                 \t  BLEU\t  chr-F\t  TER\tTime\n",
      "tatoeba-test-v2021-08-07\t  19.8\t   48.8\t 67.4\t79.03s\n",
      "flores101-devtest       \t  29.6\t   60.7\t 57.1\t16.25s\n",
      "tico19-test             \t  30.2\t   60.5\t 57.1\t53.23s\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import json\n",
    "from tabulate import tabulate\n",
    "\n",
    "# Define test sets and pre-recorded times\n",
    "tests = [\n",
    "    {\n",
    "        \"name\": \"tatoeba-test-v2021-08-07\",\n",
    "        \"ref\": \"/kaggle/input/bleu-test/ref1.txt\",\n",
    "        \"hyp\": \"/kaggle/working/tatoeba-translated.txt\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"flores101-devtest\",\n",
    "        \"ref\": \"/kaggle/input/flores101-enar/ara.devtest\",\n",
    "        \"hyp\": \"/kaggle/working/flores101-translated.txt\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"tico19-test\",\n",
    "        \"ref\": \"/kaggle/input/tico19-enar/ref.ar\",\n",
    "        \"hyp\": \"/kaggle/working/tico-translated.txt\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# Function to run sacrebleu and extract metrics\n",
    "def get_scores_json(ref, hyp):\n",
    "    result = subprocess.run(\n",
    "        [\"sacrebleu\", ref, \"-i\", hyp, \"-m\", \"bleu\", \"chrf\", \"ter\"],\n",
    "        capture_output=True,\n",
    "        text=True\n",
    "    )\n",
    "    if result.returncode != 0:\n",
    "        print(f\"Error: {result.stderr}\")\n",
    "        return None\n",
    "    try:\n",
    "        data = json.loads(result.stdout)\n",
    "        scores = {entry[\"name\"]: entry[\"score\"] for entry in data}\n",
    "        return scores\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to parse JSON: {e}\")\n",
    "        return None\n",
    "\n",
    "# Build table\n",
    "full_table = []\n",
    "\n",
    "for i, test in enumerate(tests):\n",
    "    scores = get_scores_json(test[\"ref\"], test[\"hyp\"])\n",
    "    time_str = f\"{(times[i]):.2f}s\"\n",
    "    if scores:\n",
    "        full_table.append([\n",
    "            test[\"name\"],\n",
    "            f\"{scores.get('BLEU', 0):.1f}\",\n",
    "            f\"{scores.get('chrF2', 0):.1f}\",\n",
    "            f\"{scores.get('TER', 0):.1f}\",\n",
    "            time_str\n",
    "        ])\n",
    "    else:\n",
    "        full_table.append([test[\"name\"], \"N/A\", \"N/A\", \"N/A\", time_str])\n",
    "\n",
    "# Print result\n",
    "print(f\"Model:{model_name}\")\n",
    "headers = [\"testset\", \"BLEU\", \"chr-F\", \"TER\", \"Time\"]\n",
    "print(tabulate(full_table, headers=headers, tablefmt=\"tsv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 6646653,
     "sourceId": 10722250,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6657619,
     "sourceId": 10737194,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6657624,
     "sourceId": 11663036,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31012,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
